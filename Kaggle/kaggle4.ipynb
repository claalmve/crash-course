{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUSA CX Kaggle Capstone Project\n",
    "## Part 4: Deep Learning in Keras and Submitting to Kaggle\n",
    "\n",
    "### Table Of Contents\n",
    "* [Introduction](#section1)\n",
    "* [Initial Setup](#section2)\n",
    "* [Deep Learning](#section3)\n",
    "* [Final Kaggle Evaluation](#section4)   \n",
    "* [Conclusion](#conclusion)\n",
    "* [Additional Reading](#reading)\n",
    "\n",
    "\n",
    "### Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Originally authored by [Patrick Chao](mailto:prc@berkeley.edu) & [Arun Ramamurthy](mailto:contact@arun.run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# SUSA CX Kaggle Capstone Project\n",
    "\n",
    "Woohoo! You've made it to the end of the CX Kaggle Capstone Project! Congratulations on all of your hard work so far. We hope you've enjoyed this opportunity to learn new modeling techniques, some underlying mathematics, and even make new friends within CX. At this point, we've covered the entirety of the Data Science Workflow, linear regression, feature engineering, PCA, shrinkage, hyperparameter tuning, decision trees and even ensemble models. This week, we're going to finish off this whirlwind tour with a revisit to our old friend, Deep Learning. While the MNIST digit dataset was really interesting to look at as a cool toy example of the powers of DL, this time you're going to apply neural networks to your housing dataset for some hands-on practice using Keras. \n",
    "\n",
    "> ### CX Kaggle Competition & Final Kaggle Evaluation\n",
    "After you get some practice with deep learning, this week we will be asking you and your team to select and finalize your best model, giving you the codespace to write up your finalized model and evaluate it by officially submitting your results to Kaggle. The winners of this friendly collab-etition will be honored at the SUSA Banquet next Friday, including prizes for the winning team! We also want to encourage and facilitate discussion between teams on why different models performed differently, and give you a chance to chat with other teams about their own experiences with the CX Kaggle Capstone. \n",
    "\n",
    "## Logistics\n",
    "\n",
    "Most of the logistics are the same as last week, but we are repeating them here for your convenience. Please let us know if you or your teammates are feeling nervous about the pace of this project - remember that we are not grading you on your project, and we really try to make the notebooks relatively easy and fast to code through. If for any reason you are feeling overwhelmed or frustrated, please DM us or talk to us in person. We want all of you to have a productive, healthy, and fun time learning data science! If you have any suggestions or recommendations on how to improve, please do not hesitate to reach out!\n",
    "\n",
    "\n",
    "### Mandatory Office Hours\n",
    "\n",
    "Because this is such a large project, you and your team will surely have to work on it outside of meetings. In order to get you guys to seek help from this project, we are making it **mandatory** for you and your group to attend **two (2)** SUSA Office Hours over the next 4 weeks. This will allow questions to be answered outside of the regular meetings and will help promote collaboration with more experienced SUSA members.\n",
    "\n",
    "The schedule of SUSA office hours are below:\n",
    "https://susa.berkeley.edu/calendar#officehours-table\n",
    "\n",
    "We understand that most of you will end up going to Arun or Patrick's office hours, but we highly encourage you to go to other people's office hours as well. There are many qualified SUSA mentors who can help and this could be an opportunity for you to meet them.\n",
    "\n",
    "<a id='section2'></a>\n",
    "# Initial Setup\n",
    "\n",
    "To begin we will import all the necessary libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "from sklearn import tree # There are lots of other models from this module you can try!\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, Ridge\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "sqrt=np.sqrt\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a few familiar functions that should be helpful to you later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data, col_list, y_name):\n",
    "    \"\"\"\n",
    "    Function to return a numpy matrix of pandas dataframe features, given k column names and a single y column\n",
    "    Outputs X, a n X k dimensional numpy matrix, and Y, an n X 1 dimensional numpy matrix.\n",
    "    This is not a smart function - although it does drop rows with NA values. It might break. \n",
    "    \n",
    "    data(DataFrame): e.g. train, clean\n",
    "    col_list(list): list of columns to extract data from\n",
    "    y_name(string): name of the column you to treat as the y column\n",
    "    \n",
    "    Ideally returns one np.array of shape (len(data), len(col_list)), and one of shape (len(data), len(col_list))\n",
    "    \"\"\"\n",
    "    \n",
    "    # keep track of numpy values\n",
    "    feature_matrix = data[col_list + [y_name]].dropna().values\n",
    "    np.random.shuffle(feature_matrix)\n",
    "    return feature_matrix[:, :-1], feature_matrix[:, -1]\n",
    "\n",
    "def get_loss(model, X,Y_true):\n",
    "    \"\"\"Returns square root of L2 loss (RMSE) from a model, X value input, and true y values\n",
    "    \n",
    "    model(Model object): model we use to predict values\n",
    "    X: numpy matrix of x values\n",
    "    Y_true: numpy matrix of true y values\n",
    "    \"\"\"\n",
    "    Y_hat = model.predict(X)\n",
    "    return get_RMSE(Y_hat,Y_true)\n",
    "\n",
    "def get_RMSE(Y_hat,Y_true):\n",
    "    \"\"\"Returns square root of L2 loss (RMSE) between Y_hat and true values\n",
    "    \n",
    "    Y_true: numpy matrix of predicted y values\n",
    "    Y_true: numpy matrix of true y values\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((Y_true-Y_hat)**2))\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def get_train_and_val(X,Y):\n",
    "    \"\"\"Given the X and Y data, return the training and validation based on the split variable\n",
    "    \n",
    "    X: numpy matrix of x values\n",
    "    Y: numpy matrix of y values\n",
    "    split: value between 0 and 1 for the training split\n",
    "    \"\"\"\n",
    "    \n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "\n",
    "    train_index,_ = get_train_val_indices(X,Y)\n",
    "\n",
    "    y_train = Y.reshape(Y.shape[0],)\n",
    "    y_train = Y[:train_index]\n",
    "    x_train = X[:train_index,:]\n",
    "\n",
    "    x_val = X[train_index:,:]\n",
    "    y_val = Y[train_index:]\n",
    "    return (x_train,y_train),(x_val,y_val)\n",
    "\n",
    "def get_train_val_indices(X,Y=None,split=0.7):\n",
    "    train_index = (int)(X.shape[0]*split)\n",
    "    test_index =X.shape[0]-1\n",
    "    return train_index,test_index\n",
    "\n",
    "def select_columns_except(dframe, non_examples):\n",
    "    \"\"\"Returns all comlumns in dframe except those in non_examples.\"\"\"\n",
    "    all_cols = dframe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cond = lambda x: sum([x == col for col in non_examples]) >= 1\n",
    "    return [x for x in all_cols if not cond(x)]\n",
    "\n",
    "#Metric for keras\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "First, we need to load and clean the data. Although you may have cleaned data from `kaggle1`, we provide our solution for the cleaned housing data for your convenience. If you would like to view the completed data cleaning procedure, it has been updated in [kaggle1.ipynb](kaggle1.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>...</th>\n",
       "      <th>ExteriorWd Shng</th>\n",
       "      <th>ExteriorImStucc</th>\n",
       "      <th>ExteriorWdShing</th>\n",
       "      <th>ExteriorMetalSd</th>\n",
       "      <th>ExteriorCmentBd</th>\n",
       "      <th>ExteriorCemntBd</th>\n",
       "      <th>ExteriorPlywood</th>\n",
       "      <th>ExteriorBrkFace</th>\n",
       "      <th>ExteriorWd Sdng</th>\n",
       "      <th>ExteriorBrk Cmn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>706.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>486.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>655.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  BedroomAbvGr  BsmtCond  BsmtExposure  BsmtFinSF1  \\\n",
       "0       856       854             3         3             1       706.0   \n",
       "1      1262         0             3         3             4       978.0   \n",
       "2       920       866             3         3             2       486.0   \n",
       "3       961       756             3         4             1       216.0   \n",
       "4      1145      1053             4         3             3       655.0   \n",
       "\n",
       "   BsmtFinSF2  BsmtFinType1  BsmtFinType2  BsmtFullBath       ...         \\\n",
       "0         0.0             6             1           1.0       ...          \n",
       "1         0.0             5             1           0.0       ...          \n",
       "2         0.0             6             1           1.0       ...          \n",
       "3         0.0             5             1           1.0       ...          \n",
       "4         0.0             6             1           1.0       ...          \n",
       "\n",
       "   ExteriorWd Shng  ExteriorImStucc  ExteriorWdShing  ExteriorMetalSd  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                1   \n",
       "2                0                0                0                0   \n",
       "3                1                0                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   ExteriorCmentBd  ExteriorCemntBd  ExteriorPlywood  ExteriorBrkFace  \\\n",
       "0                0                0                0                0   \n",
       "1                0                0                0                0   \n",
       "2                0                0                0                0   \n",
       "3                0                0                0                0   \n",
       "4                0                0                0                0   \n",
       "\n",
       "   ExteriorWd Sdng  ExteriorBrk Cmn  \n",
       "0                0                0  \n",
       "1                0                0  \n",
       "2                0                0  \n",
       "3                1                0  \n",
       "4                0                0  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('DATA/house-prices/train_cleaned.csv')\n",
    "test = pd.read_csv('DATA/house-prices/test_cleaned.csv')\n",
    "train = train.drop('Unnamed: 0',axis=1)\n",
    "test = test.drop('Unnamed: 0',axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before, we need to preprocess the data into `numpy` matrices and separate the `SalePrice` as the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = select_columns_except(train, ['Id','SalePrice'])\n",
    "\n",
    "X, Y = get_features(train, feature_cols, 'SalePrice')\n",
    "(x_train,y_train),(x_val,y_val) = get_train_and_val(X,Y)\n",
    "\n",
    "x_test = test.loc[:, test.columns != 'Id'].values\n",
    "test_ids = test['Id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function `model_prediction` that takes in a model and a set of features from the test set, and outputs the predictions into a vector. This should work with the `keras` neural networks, `sklearn` decision trees and random forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(model,test=x_test):\n",
    "    prediction = model.predict(test)\n",
    "    return prediction.reshape(prediction.shape[0],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Example 1 : Random Forest\n",
    "To help you get started, we supply a couple of naive models. Recall the three steps to modeling: model selection, training, and evaluation (validation or testing). Use the optimal parameters you found from grid search last week to tune the following random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### MODEL DESIGN ###\n",
    "####################\n",
    "max_depth = 20\n",
    "min_samples_leaf = 1\n",
    "min_samples_split = 2\n",
    "n_estimators = 100\n",
    "model_rf = RandomForestRegressor(max_depth = max_depth,\n",
    "                              min_samples_leaf = min_samples_leaf, min_samples_split = min_samples_split,\n",
    "                              n_estimators = n_estimators, random_state = 0, bootstrap = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### TRAINING ###\n",
    "################\n",
    "model_rf = model_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(model,x_test=x_test):\n",
    "    prediction = model.predict(x_test)\n",
    "    return prediction.reshape(prediction.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error loss on the Validation Set for our RF model: 30595.99\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "### EVALUATION ###\n",
    "##################\n",
    "loss = get_loss(model_rf, x_val,y_val)\n",
    "print(\"Root Mean Squared Error loss on the Validation Set for our RF model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# Deep Learning\n",
    "\n",
    "From `kaggle3`, a strange tale:  \n",
    ">We may imagine hyperparameters as a bunch of individual knobs we can turn to change our model. Consider that you are visiting your friend and staying at her place. However, you did not realize that she is actually an alien and her house is filled with very strange objects. When you head to bed, you attempt to use her shower, but see that her shower is has a dozen of knobs that control the temperature of the water coming out! We only have a single output to work off of, but many different knobs or *parameters* to adjust. If the water is too hot, we can turn random knobs until it becomes cold, and learn a bit about our environment. We may determine that some knobs are more or less sensitive, just like hyperparameters. Each knob in the shower is equivalent to a hyperparameter we can tune in a model.\n",
    "\n",
    "## Model Example 2 : Neural Networks\n",
    "Here is a very simple example of a neural network in Keras. It's performance is not fantastic to start, but mess around with tuning parameters and adding or subtracting layers and see what you can come up with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### MODEL DESIGN ###\n",
    "####################\n",
    "model_nn = Sequential()\n",
    "model_nn.add(Dense(100, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model_nn.add(Dense(50, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model_nn.add(Dense(50, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model_nn.add(Dense(25, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model_nn.add(Dense(10, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model_nn.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_nn.compile(optimizer=Adam(), loss = root_mean_squared_error, \n",
    "              metrics =[root_mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1021 samples, validate on 439 samples\n",
      "Epoch 1/200\n",
      "1021/1021 [==============================] - 0s 448us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 2/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7159 - root_mean_squared_error: 179125.7159 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 3/200\n",
      "1021/1021 [==============================] - 0s 87us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 4/200\n",
      "1021/1021 [==============================] - 0s 84us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 5/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 6/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7180 - root_mean_squared_error: 179125.7180 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 7/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 8/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 9/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 10/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7176 - root_mean_squared_error: 179125.7176 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 11/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 12/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 13/200\n",
      "1021/1021 [==============================] - 0s 88us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 14/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 15/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 16/200\n",
      "1021/1021 [==============================] - 0s 80us/step - loss: 179125.7177 - root_mean_squared_error: 179125.7177 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 17/200\n",
      "1021/1021 [==============================] - 0s 86us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 18/200\n",
      "1021/1021 [==============================] - 0s 87us/step - loss: 179125.7154 - root_mean_squared_error: 179125.7154 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 19/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 20/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 21/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7184 - root_mean_squared_error: 179125.7184 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 22/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 23/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 24/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 25/200\n",
      "1021/1021 [==============================] - 0s 87us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 26/200\n",
      "1021/1021 [==============================] - 0s 98us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 27/200\n",
      "1021/1021 [==============================] - 0s 86us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 28/200\n",
      "1021/1021 [==============================] - 0s 84us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 29/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7179 - root_mean_squared_error: 179125.7179 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 30/200\n",
      "1021/1021 [==============================] - 0s 80us/step - loss: 179125.7156 - root_mean_squared_error: 179125.7156 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 31/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7158 - root_mean_squared_error: 179125.7158 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 32/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 33/200\n",
      "1021/1021 [==============================] - 0s 67us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 34/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7176 - root_mean_squared_error: 179125.7176 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 35/200\n",
      "1021/1021 [==============================] - 0s 79us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 36/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 37/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 38/200\n",
      "1021/1021 [==============================] - 0s 85us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 39/200\n",
      "1021/1021 [==============================] - 0s 88us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 40/200\n",
      "1021/1021 [==============================] - 0s 83us/step - loss: 179125.7176 - root_mean_squared_error: 179125.7176 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 41/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021/1021 [==============================] - 0s 84us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 43/200\n",
      "1021/1021 [==============================] - 0s 89us/step - loss: 179125.7161 - root_mean_squared_error: 179125.7161 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 44/200\n",
      "1021/1021 [==============================] - 0s 88us/step - loss: 179125.7161 - root_mean_squared_error: 179125.7161 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 45/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 46/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 47/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 48/200\n",
      "1021/1021 [==============================] - 0s 79us/step - loss: 179125.7180 - root_mean_squared_error: 179125.7180 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 49/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7180 - root_mean_squared_error: 179125.7180 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 50/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 51/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 52/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7182 - root_mean_squared_error: 179125.7182 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 53/200\n",
      "1021/1021 [==============================] - 0s 72us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 54/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 55/200\n",
      "1021/1021 [==============================] - 0s 84us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 56/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 57/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7182 - root_mean_squared_error: 179125.7182 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 58/200\n",
      "1021/1021 [==============================] - 0s 68us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 59/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 60/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7161 - root_mean_squared_error: 179125.7161 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 61/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7159 - root_mean_squared_error: 179125.7159 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 62/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 63/200\n",
      "1021/1021 [==============================] - 0s 68us/step - loss: 179125.7180 - root_mean_squared_error: 179125.7180 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 64/200\n",
      "1021/1021 [==============================] - 0s 69us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 65/200\n",
      "1021/1021 [==============================] - 0s 72us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 66/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7159 - root_mean_squared_error: 179125.7159 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 67/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 68/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7150 - root_mean_squared_error: 179125.7150 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 69/200\n",
      "1021/1021 [==============================] - 0s 80us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 70/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7180 - root_mean_squared_error: 179125.7180 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 71/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7159 - root_mean_squared_error: 179125.7159 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 72/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7181 - root_mean_squared_error: 179125.7181 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 73/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 74/200\n",
      "1021/1021 [==============================] - 0s 80us/step - loss: 179125.7179 - root_mean_squared_error: 179125.7179 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 75/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7177 - root_mean_squared_error: 179125.7177 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 76/200\n",
      "1021/1021 [==============================] - 0s 69us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 77/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 78/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 79/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 80/200\n",
      "1021/1021 [==============================] - 0s 68us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 81/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 82/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7159 - root_mean_squared_error: 179125.7159 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 83/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 85/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7176 - root_mean_squared_error: 179125.7176 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 86/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7180 - root_mean_squared_error: 179125.7180 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 87/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7154 - root_mean_squared_error: 179125.7154 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 88/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7179 - root_mean_squared_error: 179125.7179 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 89/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 90/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7156 - root_mean_squared_error: 179125.7156 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 91/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 92/200\n",
      "1021/1021 [==============================] - 0s 79us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 93/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 94/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7161 - root_mean_squared_error: 179125.7161 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 95/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 96/200\n",
      "1021/1021 [==============================] - 0s 69us/step - loss: 179125.7150 - root_mean_squared_error: 179125.7150 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 97/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 98/200\n",
      "1021/1021 [==============================] - 0s 65us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 99/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 100/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 101/200\n",
      "1021/1021 [==============================] - 0s 72us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 102/200\n",
      "1021/1021 [==============================] - 0s 72us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 103/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7179 - root_mean_squared_error: 179125.7179 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 104/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7158 - root_mean_squared_error: 179125.7158 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 105/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7176 - root_mean_squared_error: 179125.7176 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 106/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 107/200\n",
      "1021/1021 [==============================] - 0s 72us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 108/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 109/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 110/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 111/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 112/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 113/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 114/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 115/200\n",
      "1021/1021 [==============================] - 0s 67us/step - loss: 179125.7159 - root_mean_squared_error: 179125.7159 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 116/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7179 - root_mean_squared_error: 179125.7179 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 117/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7181 - root_mean_squared_error: 179125.7181 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 118/200\n",
      "1021/1021 [==============================] - 0s 88us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 119/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 120/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 121/200\n",
      "1021/1021 [==============================] - 0s 72us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 122/200\n",
      "1021/1021 [==============================] - 0s 79us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 123/200\n",
      "1021/1021 [==============================] - 0s 86us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 124/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 125/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021/1021 [==============================] - 0s 67us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 126/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7176 - root_mean_squared_error: 179125.7176 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 127/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 128/200\n",
      "1021/1021 [==============================] - 0s 72us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 129/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7161 - root_mean_squared_error: 179125.7161 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 130/200\n",
      "1021/1021 [==============================] - 0s 86us/step - loss: 179125.7177 - root_mean_squared_error: 179125.7177 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 131/200\n",
      "1021/1021 [==============================] - 0s 96us/step - loss: 179125.7177 - root_mean_squared_error: 179125.7177 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 132/200\n",
      "1021/1021 [==============================] - 0s 83us/step - loss: 179125.7180 - root_mean_squared_error: 179125.7180 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 133/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 134/200\n",
      "1021/1021 [==============================] - 0s 85us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 135/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 136/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7158 - root_mean_squared_error: 179125.7158 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 137/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 138/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 139/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7153 - root_mean_squared_error: 179125.7153 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 140/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7176 - root_mean_squared_error: 179125.7176 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 141/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 142/200\n",
      "1021/1021 [==============================] - 0s 83us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 143/200\n",
      "1021/1021 [==============================] - 0s 80us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 144/200\n",
      "1021/1021 [==============================] - 0s 83us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 145/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 146/200\n",
      "1021/1021 [==============================] - 0s 79us/step - loss: 179125.7151 - root_mean_squared_error: 179125.7151 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 147/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7176 - root_mean_squared_error: 179125.7176 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 148/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 149/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7180 - root_mean_squared_error: 179125.7180 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 150/200\n",
      "1021/1021 [==============================] - 0s 89us/step - loss: 179125.7179 - root_mean_squared_error: 179125.7179 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 151/200\n",
      "1021/1021 [==============================] - 0s 91us/step - loss: 179125.7180 - root_mean_squared_error: 179125.7180 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 152/200\n",
      "1021/1021 [==============================] - 0s 83us/step - loss: 179125.7179 - root_mean_squared_error: 179125.7179 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 153/200\n",
      "1021/1021 [==============================] - 0s 91us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 154/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 155/200\n",
      "1021/1021 [==============================] - 0s 95us/step - loss: 179125.7151 - root_mean_squared_error: 179125.7151 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 156/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 157/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7179 - root_mean_squared_error: 179125.7179 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 158/200\n",
      "1021/1021 [==============================] - 0s 85us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 159/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 160/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 161/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7182 - root_mean_squared_error: 179125.7182 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 162/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 163/200\n",
      "1021/1021 [==============================] - 0s 80us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 164/200\n",
      "1021/1021 [==============================] - 0s 78us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 165/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021/1021 [==============================] - 0s 69us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 167/200\n",
      "1021/1021 [==============================] - 0s 80us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 168/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 169/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 170/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 171/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7158 - root_mean_squared_error: 179125.7158 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 172/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 173/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7177 - root_mean_squared_error: 179125.7177 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 174/200\n",
      "1021/1021 [==============================] - 0s 87us/step - loss: 179125.7173 - root_mean_squared_error: 179125.7173 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 175/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 176/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 177/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 178/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7162 - root_mean_squared_error: 179125.7162 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 179/200\n",
      "1021/1021 [==============================] - 0s 76us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 180/200\n",
      "1021/1021 [==============================] - 0s 91us/step - loss: 179125.7165 - root_mean_squared_error: 179125.7165 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 181/200\n",
      "1021/1021 [==============================] - 0s 86us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 182/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 183/200\n",
      "1021/1021 [==============================] - 0s 74us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 184/200\n",
      "1021/1021 [==============================] - 0s 73us/step - loss: 179125.7154 - root_mean_squared_error: 179125.7154 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 185/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7159 - root_mean_squared_error: 179125.7159 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 186/200\n",
      "1021/1021 [==============================] - 0s 80us/step - loss: 179125.7164 - root_mean_squared_error: 179125.7164 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 187/200\n",
      "1021/1021 [==============================] - 0s 80us/step - loss: 179125.7174 - root_mean_squared_error: 179125.7174 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 188/200\n",
      "1021/1021 [==============================] - 0s 75us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 189/200\n",
      "1021/1021 [==============================] - 0s 71us/step - loss: 179125.7190 - root_mean_squared_error: 179125.7190 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 190/200\n",
      "1021/1021 [==============================] - 0s 77us/step - loss: 179125.7167 - root_mean_squared_error: 179125.7167 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 191/200\n",
      "1021/1021 [==============================] - 0s 85us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 192/200\n",
      "1021/1021 [==============================] - 0s 90us/step - loss: 179125.7150 - root_mean_squared_error: 179125.7150 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 193/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 194/200\n",
      "1021/1021 [==============================] - 0s 83us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 195/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7184 - root_mean_squared_error: 179125.7184 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 196/200\n",
      "1021/1021 [==============================] - 0s 83us/step - loss: 179125.7170 - root_mean_squared_error: 179125.7170 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 197/200\n",
      "1021/1021 [==============================] - 0s 82us/step - loss: 179125.7179 - root_mean_squared_error: 179125.7179 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 198/200\n",
      "1021/1021 [==============================] - 0s 81us/step - loss: 179125.7177 - root_mean_squared_error: 179125.7177 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 199/200\n",
      "1021/1021 [==============================] - 0s 84us/step - loss: 179125.7168 - root_mean_squared_error: 179125.7168 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n",
      "Epoch 200/200\n",
      "1021/1021 [==============================] - 0s 70us/step - loss: 179125.7171 - root_mean_squared_error: 179125.7171 - val_loss: 185097.0019 - val_root_mean_squared_error: 185097.0019\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "### TRAINING ###\n",
    "################\n",
    "batch_size = 30\n",
    "epochs = 200\n",
    "learning_rate = 0.5\n",
    "history = model_nn.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error loss on the Validation Set for our neural net model: 185097.00\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "### EVALUATION ###\n",
    "##################\n",
    "score = model_nn.evaluate(x_val, y_val, verbose=0)\n",
    "print(\"Root Mean Squared Error loss on the Validation Set for our neural net model: {:.2f}\".format(score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# Final Kaggle Evaluation\n",
    "\n",
    "Congrats on finishing the last of the models we planned on teaching you about during the CX Kaggle Capstone! \n",
    "\n",
    "You have now covered five distinct models and a several related techniques to add to your data science bag-of-tricks: \n",
    "- Linear Models\n",
    "    - Multivariate Linear Regression\n",
    "    - Polynomial Regression\n",
    "    - Shrinkage / Biased Regression / Regularization (i.e. Ridge, LASSO)\n",
    "- Decision Trees\n",
    "    - Random Forests\n",
    "- Deep Learning\n",
    "    - Sequential Neural Networks\n",
    "- Auxiliary Techniques \n",
    "    - The Data Science Workflow\n",
    "    - Data Cleaning\n",
    "    - Interpreting EDA Graphs\n",
    "    - Feature Engineering\n",
    "    - Principal Component Analysis\n",
    "    - Hyperparameter Tuning (i.e. grid search)\n",
    "    - Ensemble Learning (i.e. bagging, boosting) \n",
    "    \n",
    "Wow, that's a lot! We are really proud of you all for exploring these techniques, which constitute some of Berkeley's toughest machine learning and statistics classes. As always, if you want to learn more about any of these topics, or are hungry to learn about even more techniques, feel free to reach out to any one of the SUSA Mentors.\n",
    "\n",
    "With the help of the above listing and your own team's preferences, choose a model and a couple of techniques to implement for your final model. We will provide you with a preamble and some space to construct and train your model, as well as a helper function to turn your output into an official Kaggle submission file.  \n",
    "\n",
    "A huge part of the modeling process is to mess around with different models, approaches, and hyperparameters! Don't be afraid to get your hands dirty and explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24898.690418755177\n"
     ]
    }
   ],
   "source": [
    "# trying to do xgboost lol\n",
    "boost = xgb.XGBRegressor(n_estimators=280, learning_rate=0.1, max_depth=3, colsample_bytree=1, reg_alpha=150,gamma=0.3, subsample=1)\n",
    "boost.fit(x_train,y_train)\n",
    "loss = get_loss(boost, x_val,y_val)\n",
    "print(loss)\n",
    "\n",
    "# for i in np.arange(1, 1000, 100):\n",
    "#     avg_loss = []\n",
    "#     for j in np.arange(5):\n",
    "#         boost = xgb.XGBRegressor(n_estimators=i, learning_rate=0.05, gamma=0.3, subsample=1,\n",
    "#                                colsample_bytree=1, max_depth=6, reg_alpha=150)\n",
    "#         boost.fit(x_train, y_train)\n",
    "#         loss = get_loss(boost, x_val, y_val)\n",
    "#         avg_loss = np.append(avg_loss, loss)\n",
    "#     print(np.mean(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################\n",
    "### PREAMBLE ###\n",
    "################\n",
    "train = x_train\n",
    "labels = y_train\n",
    "test = x_test \n",
    "\n",
    "####################\n",
    "### MODEL DESIGN ###\n",
    "####################\n",
    "model = boost\n",
    "# ^^ REPLACE THIS LINE ^^\n",
    "\n",
    "################\n",
    "### TRAINING ###\n",
    "################\n",
    "# model = model.fit(train, labels)\n",
    "# ^^ REPLACE THIS LINE ^^\n",
    "\n",
    "##################\n",
    "### EVALUATION ###\n",
    "##################\n",
    "test_predictions = model_prediction(model)\n",
    "\n",
    "##################\n",
    "### SUBMISSION ###\n",
    "##################\n",
    "def generate_kaggle_submission(predictions, test = test):\n",
    "    '''\n",
    "    This function accepts your 1459-dimensional vector of predicted SalesPrices for the test dataset, \n",
    "    and writes a CSV named kaggle_submission.csv containing your vector in a form suitable for \n",
    "    submission onto the Kaggle leaderboard.\n",
    "    '''\n",
    "    pd.DataFrame({'Id': test_ids, 'SalePrice': predictions}) \\\n",
    "      .to_csv('kaggle_submission.csv', index=False)\n",
    "    \n",
    "generate_kaggle_submission(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed in the code block above, we had to write a simple CSV file containing row IDs and predicted values for the 1459 houses in the test dataset. This submission file is your ticket to getting onto the official Kaggle leaderboard and seeing how you did as compared to the rest of the world! \n",
    "\n",
    "Take a look at your `kaggle_submission.csv` file to ensure its content matches your expectations. When you and your team are ready, follow these instructions to upload your predictions to Kaggle and receive an official Kaggle score:\n",
    "\n",
    "> 1. First, choose one person on your team (perhaps the rprincess) to submit your team's predictions under their name. This person will need to visit the [Kaggle website](https://www.kaggle.com) to create an account.\n",
    "> 2. Go to the [Competition Submission Page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submit) for the House Prices competition. \n",
    "> 3. Upload and submit `kaggle_submission.csv`\n",
    "> 4. Wait for your submission to be scored! This should only take a few seconds to submit. You may have to refresh the page to see your final score.\n",
    "> 5. To each member: record your team's score in [this Google form](https://goo.gl/forms/VteqJ5Di84t54TEZ2), which also contains a section for feedback on your Kaggle experience this semester. Congratulations on your first Kaggle submission!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For initial comparison, the naive random forest in this notebook achieves a score of 0.15-0.2 - you can think of this as a baseline score to beat.   \n",
    "\n",
    "This is Patrick's score after some minor edits to the models above:\n",
    "\n",
    "<img src=\"GRAPHICS/kaggle.png\" width=\"80%\">\n",
    "\n",
    "Can you beat Patrick's Kaggle score?\n",
    "> \"As an incentive, if you get a score under 0.13, I will personally take your team out to lunch!\" - Patrick Chao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get some great models and great kaggle scores, feel free to screenshot them and post them in the slack!\n",
    "\n",
    "We will post a leaderboard of all of the CX Kaggle teams, and the winning team will be honored at banquet, so stay tuned for that! \n",
    "\n",
    "In the meantime, take some time to talk to other finished teams and explore the differences in your modeling approach, design, and insights! Happy Kaggling :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This brings us to an end to the CX Kaggle Capstone Project, as well as the Spring 2018 semester of SUSA Career Exploration. Congratulations on graduating from the SUSA Career Exploration committee! It's been a wonderful experience teaching you all, and we hope you got as much out of CX as we did this semester. This semester brought several new pilot programs to CX, such as the crash courses, workbooks, a revamped curriculum, and the CX Kaggle Capstone Project. You all have been great sources of feedback, and we want to make next semester's CX curriculum even better for the new generation of CX! \n",
    "\n",
    "We're going to ask you for feedback one last time, to give us insight into how we can improve the CX Kaggle Capstone experience for future CX members. Please fill out [this feedback form](https://goo.gl/forms/VteqJ5Di84t54TEZ2) and let us know how we could have done better. Thank you again for a wonderful semester, and we will see you again in the Fall!\n",
    "\n",
    "As always, please email [Arun Ramamurthy](mailto:contact@arun.run), [Patrick Chao](mailto:prc@berkeley.edu), or [Noah Gundotra](mailto:noah.gundotra@berkeley.edu) with any questions or concerns whatsoever. Have a great summer, and we hope to see you as a returning member in the Fall! Go SUSA!!!\n",
    "\n",
    "**Signed with geom_love,  \n",
    "Lucas, Arun, Patrick, Noah, and the rest of the SUSA Board**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
