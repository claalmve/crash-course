{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUSA CX Kaggle Capstone Project\n",
    "## Part 2: Feature Engineering, Shrinkage, and PCA in Linear Models\n",
    "\n",
    "### Table Of Contents\n",
    "* [Introduction](#section1)\n",
    "* [Extensions to Linear Modeling](#section2)   \n",
    "    1. [Feature Engineering & Polynomial Regression](#i)\n",
    "    2. [Shrinkage & Lasso Regression](#ii)\n",
    "    3. [Principal Component Analysis](#iii)\n",
    "* [Conclusion](#conclusion)\n",
    "* [Additional Reading](#reading)\n",
    "\n",
    "\n",
    "### Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Originally authored by [Arun Ramamurthy](mailto:contact@arun.run), [Patrick Chao](mailto:prc@berkeley.edu), & [Noah Gundotra](mailto:noah.gundotra@berkeley.edu).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# SUSA CX Kaggle Capstone Project\n",
    "\n",
    "Welcome to the second week of the SUSA CX Kaggle Capstone! Now that you have an understanding of your data, a cleaned dataset, and a working model, now it's time to improve your model performance with some new tricks and techniques.  This week, you and your teammates will choose one of three techniques to improve your model further, read an article on the topic, and read through documentation to write the model for yourselves! The overarching goal of this week is to give you practice reading about and coding basic, but powerful, models - just like you'd do in real life when learning a new model design. \n",
    "\n",
    "## How will you outperform your first model?\n",
    "\n",
    "One way to improve your model performance is to use a better model. These days, deep learning models are popular because they are performant and do not suffer from model bias. However, these models also lack interpretability, clear diagnostic procedures, and replicability. In Week 4 we will help you design and train a neural net to predict `SalesPrice`, but in the meantime, we are going to be sticking with (generalized) linear models.\n",
    "\n",
    "This week, you will choose one of the following three topics to learn with your team. The topics are as follows:   \n",
    "1. **Feature Engineering**: In this module, you will learn guidelines for pre-processing your features (e.g. squaring them, or taking the log, etc.). This segues into Polynomial Regression, a form of linear regression where you apply several different powers to your features before entering them into a model.\n",
    "\n",
    "2. **Shrinkage**: In this module, you will learn a technique that specifically targets overfitting and feature selection. Instead of selecting relevant features manually, shrinkage will automatically get rid of unimportant features and avoid sinuous linear models. This segues into Lasso Regression, a form of regression that penalizes the use of too many features.\n",
    "\n",
    "2. **Principal Component Analysis**: In this module, you will learn a technique that performs both feature engineering and shrinkage at the same time by re-constructing a new basis (yes, that same word you learnt about in Math 54!) for your features. Once cast into this new basis, your dataset retains all of its information, but your new features are orthogonal and sorted in order of importance. You can then just choose the first few of these features, and your linear model should increase in predictive accuracy.\n",
    "\n",
    "# Logistics\n",
    "\n",
    "Most of the logistics are the same as last week, but we are repeating them here for your convenience. Please let us know if you or your teammates are feeling nervous about the pace of this project - remember that we are not grading you on your project, and we really try to make the notebooks relatively easy and fast to code through. If for any reason you are feeling overwhelmed or frustrated, please DM us or talk to us in person. We want all of you to have a productive, healthy, and fun time learning data science!\n",
    "\n",
    "### SUSA Datathon\n",
    "\n",
    "The SUSA Datathon is this weekend, and it's the perfect time to finish the contents of this workbook! The datathon is on **Sunday from 5-9PM, in GPB 100**, and **dinner will be provided**. This event is **mandatory** for all CX members, but many other committees and mentors in SUSA will be there too. At the Datathon, members from many SUSA committees, notably Data Consulting, Research & Publication, and Career Exploration, will all meet up and work on their respective projects together. There will be many other SUSA mentors there to help you and it should be a great environment for you and your group to work in. This is also another great opportunity to meet other experienced SUSA members and get a taste for other committees in the club. See you all there!\n",
    "\n",
    "### Mandatory Office Hours\n",
    "\n",
    "Because this is such a large project, you and your team will surely have to work on it outside of meetings. In order to get you guys to seek help from this project, we are making it **mandatory** for you and your group to attend **two (2)** SUSA Office Hours over the next 4 weeks. This will allow questions to be answered outside of the regular meetings and will help promote collaboration with more experienced SUSA members.\n",
    "\n",
    "The schedule of SUSA office hours are below:\n",
    "https://susa.berkeley.edu/calendar#officehours-table\n",
    "\n",
    "We understand that most of you will end up going to Arun or Patrick's office hours, but we highly encourage you to go to other people's office hours as well. There are many qualified SUSA mentors who can help and this could be an opportunity for you to meet them.\n",
    "\n",
    "### Git\n",
    "\n",
    "Given that this is a collaborative project, you'll need to work with your team members on the same codebase simutaneously! This is fortunately simple with Git, which you learned in your very first workshop. Visit `py0` if you need a refresher, but we will be going over the steps for collaborative work here too.\n",
    "\n",
    "1. First, decide on which one of you will be hosting the forked Github repository for `crash-course`. Ideally, this would be someone with some GitHub experience and a GitHub account. If no one on your team has a GitHub account, one of you should sign up for one. For our examples, we will call this person's account name `rprincess`.\n",
    "\n",
    "2. Next, have the above person navigate to the [SUSA crash-course repository](https://github.com/SUSA-org/crash-course) and click the `Fork` button. GitHub will make a copy of the crash-course repository in your team member's account. \n",
    "\n",
    "3. Each one of you can download the `rprincess` repository to your local computer with the following command: `git clone https://github.com/rprincess/crash-course.git`.\n",
    "\n",
    "4. The `rprincess` must add their team members' account information to the fork (to allow them to push to your repo). You can find this option by going to the Settings page of your forked repository of crash-course, then clicking \"Collaborators & Teams\" (e.g. `https://github.com/rprincess/git-demo/settings/collaboration`).\n",
    "\n",
    "4. Feel free to work within the repository and use `git pull origin` and `git add -A && git commit -am \"My example message here\" && git push` to pull/push your local repository to the `rprincess` online repository.\n",
    "\n",
    "If you have any questions, just Slack Lucas, Noah, Patrick, or Arun and we can help you with your Git workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Extensions to Linear Modeling\n",
    "\n",
    "Before we begin to explore improvements to our initial design in `kaggle1`, let's review what you and your team actually did. You cleaned a dataset of real estate sales and then selected several important features. You then fed these features into a linear model, which then tuned its weights to minimize your training set error. However, you may have noticed that your error was still astronomically high. The linear model is rarely a great fit for a complex set of relationships out-of-the-box. \n",
    "\n",
    "This week, we will show you three different ways you can improve your basic linear model from `kaggle1`. We will introduce the concepts to you, but the majority of your learning will be reading external source material and technical documentation. This is a very important skill to practice, as much of your data science education will come from packages and model extensions learned on-the-fly to solve problems. \n",
    "\n",
    "Review the potential improvements you can make with your team, and decide on one that sounds interesting to you all! Each one uses a different approach to solve two of the most common sources of poor model performance: **bias** and **variance**\n",
    "\n",
    "For your convenience, a brief summary of the three improvements you can choose between is repeated below:\n",
    "1. **Feature Engineering**: In this module, you will learn guidelines for pre-processing your features (e.g. squaring them, or taking the log, etc.). This segues into Polynomial Regression, a form of linear regression where you apply several different powers to your features prior to using them to train a linear model.\n",
    "\n",
    "2. **Shrinkage**: In this module, you will learn a technique that specifically targets overfitting and feature selection. Instead of selecting relevant features manually, shrinkage will automatically get rid of unimportant features and avoid sinuous linear models. This segues into Lasso Regression, a form of regression that penalizes the use of too many features.\n",
    "\n",
    "3. **Principal Component Analysis**: In this module, you will learn a technique that performs both feature engineering and shrinkage at the same time by re-constructing a new basis (yes, that same word you learnt about in Math 54!) for your features. Once cast into this new basis, your dataset retains all of its information, but your new features are orthogonal and sorted in order of importance. You can then just choose the first few of these features, and your linear model should increase in predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Can you intuit which techniques deal with bias? Which deal with variance? (*Hint: Underfitting is caused by bias, and overfitting is caused by variance.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='i'></a>\n",
    "# 1. Feature Engineering & Polynomial Regression\n",
    "\n",
    "Why was the initial linear model so poor in the first place? While the linear regression model is wonderfully fitted with statistically interpretable weights, it also ships with several very strong assumptions. (For more on these assumptions, visit the `r3` notebook!) In particular, linear models perform poorly on features that aren't *widely spread*, and are particularily *sensitive to outliers*. \n",
    "\n",
    "While there are many approaches to improving a model's design to make it more *robust* to outliers, an alternate approach would be to fix, or massage, our data until it better fits the model. This is a process called **feature engineering** - the pre-processing of your data to make them more appropriate for your model. In some ways, the most basic feature engineering exercise is just removing outliers. However, this can lead to *cherry-picking*, or overly influencing the exact nature of the dataset. In practice, some feature engineering is informed by domain knowledge, as discussed in the `adv-pyr1` workshop. However, the shape of our data itself also tells us if, and even how, we should transform it! \n",
    "\n",
    "For example, suppose we had a vector $v = [1, 1, 2, 2, 3, 3, 100, 10000]$. This simple one-dimensional vector has two clear outliers. These outliers would greatly influence a linear model that uses $v$ as an input feature. How can we make the values in $v$ closer together, so that the outliers don't stand out quite as much?\n",
    "> Can you and your team think of a simple function like $f(x) = x^2$, $f(x) = \\frac{1}{x}$, $f(x) = \\sqrt x$ that would make $v' = f(v)$ have a more concentrated set of values?\n",
    "\n",
    "One function that is desirable for transformation is $\\log(x)$, as it preserves the order of the values fed to it. Let's try it on our vector $v$ and see what happens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v' = ['0.0', '0.0', '0.301', '0.301', '0.477', '0.477', '2.0', '4.0']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "v = [1, 1, 2, 2, 3, 3, 100, 10000]\n",
    "v_prime = [math.log10(x) for x in v]\n",
    "print(\"v' =\", ['{0:.3}'.format(x) for x in v_prime])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How neat! So it turns out that $v'$ has values ranging from 0 to 4, in contrast to the original $v$, which ranged from 1 to 10000. This is indicative of when $\\textit{log}$ is used in the wild - it pulls outliers in to make the data more even. \n",
    "\n",
    "> Can you think of a potential problem with $\\log$ as a transformation function? Apply a similar critique to $x^{-1}$, and to $\\sqrt{x}$.\n",
    "\n",
    "As an example, consider this plot of a couple features in your dataset. Notice how $\\log$ fixes the poor spread of the `PoolArea` feature. This is a classic example of how feature engineering can make your features more appropriate for a linear model.\n",
    "\n",
    "![](GRAPHICS/spread.png)\n",
    "\n",
    "> Can you use EDA in either Python or R to determine which variables have outliers? What kinds of data visualizations would help to showcase outliers? (*Hint: these are univariate displays of data*)\n",
    "> Consider applying `log` or `sqrt` to the features you find with huge outliers. \n",
    "\n",
    "However, there are many more functions than $\\log$ out there, so how do we choose between them in a more general sense? The late John Tukey developed a simple rule-of-thumb for practical statisticians: **the bulging rule**.\n",
    "\n",
    "![](https://i0.wp.com/f.hypotheses.org/wp-content/blogs.dir/253/files/2014/06/Selection_005.png?resize=295%2C318)\n",
    "\n",
    "This tool is very easy to use - if your simple relationship between some feature vector and the response variable vector is curved up, take your $x$ \"up\" the *ladder of powers* by squaring it or cubing it. Alternatively, take your $y$ \"down\" by taking the square-root or log. Either of these will straighten your data, allowing it to fit a linear model.\n",
    "\n",
    "In this quick example, the quadratic relationship between $x$ and $y$ make a simple, naive linear model highly biased (systemically off from the true answer), since it is unable to effectively characterize the relationship between $x$ and $y$. However, if we note the point cloud curves up, the bulging rule indicates we should take $x$ \"up\" the ladder of powers and into $x' = x^2$. Such a transformation actually fixes the data for use in a linear model, as the relationship is linear in terms of $x' = x^2$, even though the relationship is not linear in terms of $x$.\n",
    "\n",
    "![](GRAPHICS/engineering.png)\n",
    "\n",
    "> Can you use EDA in either Python or R to determine which variables have non-linear simple relationships with `SalesPrice`? According to the Bluging Rule, how should you engineer these features?\n",
    "\n",
    "A note on what **linear** means in linear models. In the above toy example, the first model is linear in terms of $x$, so although it misses the cloud, it looks like a line. However, the second model is actually a model of a *quadratic relationship*, but since it is linear in terms of $x' =x^2$, *it is still a linear model* and still looks like a line, as long as the x-axis is actually $x' = x^2$! So, even though Polynomial Regression is used to model non-linear relationships, the model itself is still considered linear, because it is linear in terms of these newly engineered features.\n",
    "\n",
    "Feature engineering is actually how Polynomial Regression is performed implicitly. Rather than manually deciding on some $x' = f(x)$ to edit your data to make the features more linear, you can just try $x'_{(1)} = x, x'_{(2)} = x^2, x'_{(3)} = x^3, x'_{(4)} = x^4$ all at once, and odds are that the bias of your linear model will drop sharply. However, as noted in previous workshops, having too many features or powers of features leads to overfitting. \n",
    "\n",
    "More concretely, suppose you derive that $x' = x^2$ helps fix your model, but you find keeping the $x$ and even adding an additional $x'' = x^3$ too improves your model performance. Then you would just apply those transformations prior to modeling, use a standard linear model like you used in `kaggle1`, and all of a sudden you're doing Polynomial Regression!\n",
    "\n",
    "Let's try using Polynomial Regression to improve your model!\n",
    "\n",
    "> Use EDA and the plots from `kaggle1` to determine which features have a non-linear relationship with `SalesPrice`. Use featuring engineering to increase the polynomial degree of these features as new columns in your cleaned dataframe. Save this dataset as `cleaned_polyomial.csv`. Try running this new dataset through the linear model you made in `kaggle1`. Do you see an improvement in your model error?   \n",
    "\n",
    "> Read through the following small tutorial on [Polynomial Regression](http://www.ritchieng.com/machine-learning-polynomial-regression/). Create a similar model called `poly_model`, but use `PolynomialFeatures` to do it automatically rather than manually engineering polynomials into your dataset. Run this model on your `clean` dataset. How do the results compare to your naive model, and your manual polynomial model?\n",
    "\n",
    "This weekend, we will show you a way to more accurately assess the quality of your model for both overfitting and underfitting by using the Kaggle leaderboard itself! Until then, **be wary about having polynomials of too high a degree (e.g. > 3) in your dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression and Feature Engineering in the Housing Dataset\n",
    "Now that we have investigated feature engineering, how may we apply it to our kaggle competition? We will walk you through how to utilize feature engineering in linear regression.\n",
    "\n",
    "First, we will load in the relevant functions for our linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # There are lots of other models from this module you can try!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_features(data, col_list, y_name):\n",
    "    \"\"\"\n",
    "    Function to return a numpy matrix of pandas dataframe features, given k column names and a single y column\n",
    "    Outputs X, a n X k dimensional numpy matrix, and Y, an n X 1 dimensional numpy matrix.\n",
    "    This is not a smart function - although it does drop rows with NA values. It might break. \n",
    "    \n",
    "    data(DataFrame): e.g. train, clean\n",
    "    col_list(list): list of columns to extract data from\n",
    "    y_name(string): name of the column you to treat as the y column\n",
    "    \n",
    "    Ideally returns one np.array of shape (len(data), len(col_list)), and one of shape (len(data), len(col_list))\n",
    "    \"\"\"\n",
    "    \n",
    "    # keep track of numpy values\n",
    "    feature_matrix = data[col_list + [y_name]].dropna().values\n",
    "    return feature_matrix[:, :-1], feature_matrix[:, -1]\n",
    "\n",
    "def get_loss(model, X,Y_true):\n",
    "    \"\"\"Returns square root of L2 loss (RMSE) between Y_hat and true values\n",
    "    \n",
    "    model(Model object): model we use to predict values\n",
    "    X: numpy matrix of x values\n",
    "    Y_true: numpy matrix of true y values\n",
    "    \"\"\"\n",
    "    Y_hat = model.predict(X)\n",
    "    return np.sqrt(np.mean((Y_true-Y_hat)**2))\n",
    "\n",
    "def getTrainAndVal(X,Y,split=0.8):\n",
    "    \"\"\"Given the X and Y data, return the training and validation based on the split variable\n",
    "    \n",
    "    X: numpy matrix of x values\n",
    "    Y: numpy matrix of y values\n",
    "    split: value between 0 and 1 for the training split\n",
    "    \"\"\"\n",
    "    \n",
    "    Y = Y.reshape(Y.shape[0],)\n",
    "\n",
    "    trainIndex = (int)(X.shape[0]*split)\n",
    "\n",
    "    y_train = Y.reshape(Y.shape[0],)\n",
    "    y_train = Y[:trainIndex]\n",
    "    x_train = X[:trainIndex,:]\n",
    "\n",
    "    x_val = X[trainIndex:,:]\n",
    "    y_val = Y[trainIndex:]\n",
    "    return (x_train,y_train),(x_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is to aid in selecting relevant columns, and getting the loss of the model. We may now read in our training data that we cleaned from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = pd.read_csv('DATA/house-prices/train_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, select the relevant features you would like to feed into the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subset of features\n",
    "# Fill this in yourself!\n",
    "feature_cols = ['LotArea', 'Utilities', 'OverallCond', 'BsmtFinSF1', 'MasVnrArea']\n",
    "# Alternatively select all features\n",
    "#feature_cols = [x for x in clean.select_dtypes(include=[np.number]).columns.tolist() if not (x == \"Id\" or x == \"SalePrice\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Utilities'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-8c1b18340c6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SalePrice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTrainAndVal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-2409723788eb>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(data, col_list, y_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# keep track of numpy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfeature_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeature_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2175\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1269\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['Utilities'] not in index\""
     ]
    }
   ],
   "source": [
    "X, Y = get_features(clean, feature_cols, 'SalePrice')\n",
    "print(X.shape)\n",
    "(x_train,y_train),(x_val,y_val) = getTrainAndVal(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we can consider the root mean squared error for linear regression without any feature engineering and any regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-c29475891c8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlinear_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Root Mean Squared Error loss of our model: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train, y_train)\n",
    "loss = get_loss(linear_model, x_val,y_val)\n",
    "print(\"Root Mean Squared Error loss of our model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we may now engineer our own features! To do this, we need to introduce a notion known as lambda expression. These are essentially shorthands to writing functions. If we would like to describe the function $x+3$ or $y^2$, we may write the functions f1 and f2 below. (Note, exponential functions are written using the double asterisk y**2 not the caret ^ symbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "f1 = lambda x: x+3\n",
    "f2 = lambda y: y**2\n",
    "print(f1(5))\n",
    "print(f2(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomialFeatures(df, features,functionMap,suffix,feature_cols):\n",
    "    newDf = df.copy()\n",
    "    for feature in features:\n",
    "        try:\n",
    "            newDf[feature+suffix] = newDf[feature].apply(lambda feat: functionMap(feat+1e-10))\n",
    "            if feature_cols!=None:\n",
    "                feature_cols.append(feature+suffix)\n",
    "        except:\n",
    "             print(\"Your function map failed to map for {}\".format(feature))\n",
    "    return newDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `polynomialFeatures` above takes in a dataframe, a specific dataframe, a list of features to be mapped, a function mapping, and a suffix. For example, if we would like to engineer features in the `clean` dataset, by taking the log of the features `['LotArea','BsmtFinSF1']`, we could engineer these squared features by creating features with the suffix `log` appended to the end. The code example is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>...</th>\n",
       "      <th>ConditionPlywood</th>\n",
       "      <th>ConditionMetalSd</th>\n",
       "      <th>ConditionCmentBd</th>\n",
       "      <th>ConditionWdShing</th>\n",
       "      <th>ConditionHdBoard</th>\n",
       "      <th>ConditionVinylSd</th>\n",
       "      <th>ConditionAsbShng</th>\n",
       "      <th>ConditionCemntBd</th>\n",
       "      <th>LotArealog</th>\n",
       "      <th>BsmtFinSF1log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.041922</td>\n",
       "      <td>6.559615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.169518</td>\n",
       "      <td>6.885510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.328123</td>\n",
       "      <td>6.186209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.164296</td>\n",
       "      <td>5.375278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.565214</td>\n",
       "      <td>6.484635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  MSSubClass  LotFrontage  LotArea  LotShape  Utilities  \\\n",
       "0           0          60         65.0     8450         3          3   \n",
       "1           1          20         80.0     9600         3          3   \n",
       "2           2          60         68.0    11250         2          3   \n",
       "3           3          70         60.0     9550         2          3   \n",
       "4           4          60         84.0    14260         2          3   \n",
       "\n",
       "   LandSlope  OverallQual  OverallCond  YearBuilt      ...        \\\n",
       "0          2            7            5       2003      ...         \n",
       "1          2            6            8       1976      ...         \n",
       "2          2            7            5       2001      ...         \n",
       "3          2            7            5       1915      ...         \n",
       "4          2            8            5       2000      ...         \n",
       "\n",
       "   ConditionPlywood  ConditionMetalSd  ConditionCmentBd  ConditionWdShing  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 1                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   ConditionHdBoard  ConditionVinylSd  ConditionAsbShng  ConditionCemntBd  \\\n",
       "0                 0                 1                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 1                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 1                 0                 0   \n",
       "\n",
       "   LotArealog  BsmtFinSF1log  \n",
       "0    9.041922       6.559615  \n",
       "1    9.169518       6.885510  \n",
       "2    9.328123       6.186209  \n",
       "3    9.164296       5.375278  \n",
       "4    9.565214       6.484635  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for polynomials, use two asterisks x**3\n",
    "# for log, use np.log\n",
    "# for square roots, use np.sqrt\n",
    "\n",
    "feature_cols = ['LotArea', 'Utilities', 'OverallCond', 'BsmtFinSF1', 'MasVnrArea']\n",
    "cleanEngineered = polynomialFeatures(clean,['LotArea','BsmtFinSF1'],lambda x:np.log(x),\"log\",feature_cols)\n",
    "cleanEngineered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error of our model: 67520.18\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_features(cleanEngineered, feature_cols, 'SalePrice')\n",
    "(x_train,y_train),(x_val,y_val) = getTrainAndVal(X,Y)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train, y_train)\n",
    "loss = get_loss(linear_model, x_val,y_val)\n",
    "print(\"Root Mean Squared Error of our model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the error decreased after adding a two of these log features. Try to utilize these functions to see how you can improve your loss by adding new features, but may sure you don't overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regularization\n",
    "\n",
    "Now that we have gone over feature engineering, it may be quite challenging to decide which features are actually necessary. It may be tempting to simply make quadratic or log of all the features, but this may lead to overfitting. Just as higher degree polynomials tend to overfit, the exact same happens with feature engineering.\n",
    "\n",
    "We may introduce the notion of **regularization**, in **ridge** and **lasso** regression.\n",
    "\n",
    "The main idea is that rather than the loss being the squared difference between the predicted values and real values, we may also add a regularization term. This regularization terms is some function of the weights. For ridge regression, we may take the sum of all of the squared weights in the linear regression. For lasso, it is very similar in that we take the sum of all the absolute values of the weights in the linear regression. \n",
    "\n",
    "In more mathematical terms, these are utilizing the $\\ell_2$ and $\\ell_1$ norms of the weight vectors, and adding this as a penalty to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code examples for applying ridge and lasso regression are below. Try messing with various values of alpha to see how your error changes!\n",
    "\n",
    "The value of alpha scales weights the added regularization term for the loss. Higher values of alpha means that we penalize greater weights more, and values closer to zero are more similar to unregularized linear regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error loss of ridge model: 67532.66\n"
     ]
    }
   ],
   "source": [
    "ridge_model = Ridge(alpha = 1.0)\n",
    "ridge_model.fit(x_train, y_train)\n",
    "loss = get_loss(ridge_model, x_val,y_val)\n",
    "print(\"Root Mean Squared Error of ridge model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error of lasso model: 67520.47\n"
     ]
    }
   ],
   "source": [
    "lasso_model = Lasso(alpha = 1.0)\n",
    "lasso_model.fit(x_train, y_train)\n",
    "\n",
    "loss = get_loss(lasso_model, x_val,y_val)\n",
    "print(\"Root Mean Squared Error of lasso model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to see the coefficients for your model, we may use the `.coef` field of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9hTyYk8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110c927b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace 'linear_model' with your model\n",
    "coefs = (linear_model.coef_.tolist())\n",
    "plt.hist(coefs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only give a very high level overview of ridge and lasso regression. If you have more questions, please feel free to reach out to us during office hours! The math is actually incredibly deep and very interesting. There are also these wikipedia links below. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Tikhonov_regularization\n",
    "https://en.wikipedia.org/wiki/Lasso_(statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='iii'></a>\n",
    "# 3. Principal Component Analysis\n",
    "\n",
    "You know how you've been taking Math 54 and Math 110 and EE16A and you just can't figure out for the life of you why all of this is useful? As you begin to go through more advanced statistical modeling in your Statistics classes, you will notice that **linear algebra is everything**. If it's not probability, it probably comes from linear algebra. \n",
    "\n",
    "This isn't necessarily always obvious from the early linear algebra classes at Cal. So, this week, we're going to teach you a very useful technique called **Principal Component Analysis** (*PCA*) that uses linear algebra concepts to make a new basis for your feature set. In a dataset of $k$ features, each feature or column can be considered an $n$-dimensional vector. The linear combination of these vectors forms out prediction for $y$, $\\hat{y}$, that estimates the true relationship between the feature vectors and $y$. But sometimes, the original feature vectors are not well-suited for predicting $y$ in the first place - they may suffer from **multicolinearity** and it can be unclear which subset of these vectors is strikes the right balance in the bias-variance tradeoff for optimal predictive accuracy.\n",
    "\n",
    "PCA is a pre-processing step that transforms your basis of $k$ feature vectors into a basis of $k$ **principal components** (*read: important vectors*) that span the same vector space as our original features. This new basis of principal components retains all the information from your original basis, but has splendid properties for linear modeling - (1) it removes all correlations between variables, and even (2) sorts them in order of importance in explaining $y$ (in this case `SalesPrice`). Then, you can just take the first $k' = 2, 3, 4...$ features in this new basis, and you should be able to get a very strong model for your response variable, that only has a few columns but retains most of the information.\n",
    "\n",
    "> Conceptually, in what ways does PCA reduce overfitting (variance) of your model? (*Hint: there are two. One pertains to the number of features, whereas the other pertains to highly correlated features.*)\n",
    "\n",
    "In short, PCA is a quick-and-easy way to pre-process your dataset automatically to decrease the variance component of its error. As long as your response variable (i.e. `SalesPrice`) is nearly linear in terms of the first few **principal components**, you can greatly reduce the error in your model and even improve computational time, as you're training on a few (improved) features. After conducting PCA, you can train a standard linear model on this new dataset of just a few columns, and generally get good results.\n",
    "\n",
    "This is best illustrated with practice, so let's actually use PCA to pre-process your data and conduct PCA regression!\n",
    "\n",
    "With your team, read through the following article on [concepts in PCA](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c). You do not have to understand all the linear algebra and math on eigenvectors in this article, but you should be able to answer the following questions:   \n",
    "> 1. What is dimensionality reduction? Why is it important for modeling?  \n",
    "> 2. What is the difference between feature elimination (used in `kaggle1`) and feature extraction (what you're learning about in this module)? What are the pros/cons of each?  \n",
    "> 3. Conceptually, why does PCA make our model less interpretable? Why is this important?   \n",
    "> 4. Name a couple of methods that would help us choose how many principal components to retain in the new dataset. What is the \"find-the-elbow\" method for scree plots?\n",
    "\n",
    "With your team, continue your background reading by reading the following article on [implementing and practical usage of PCA in Python using sklearn](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/). You do not have to understand the visualization code, but you should be able to answer the following questions with your team:   \n",
    "> 1. What is standardizing/normalization? Why is it an important pre-processing step for PCA?\n",
    "> 2. What is the relationship between the order of the principal components and their importance in modeling $y$?  \n",
    "> 3. Read over and try to understand the code in the implementation section, \"For Python Users\". (Optionally, if you like R, feel free to read over the R section as well!). You will be using `sklearn` to implement PCA for your own housing dataset.\n",
    "\n",
    "Now armed with your PCA dataset, let's train a linear model and compare it to our naive first attempt in `kaggle1`.\n",
    "\n",
    "> Use `sklearn`'s [PCA function](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to process your dataset into a new dataset of principal components. Save this as `pca_full.csv`.   \n",
    "\n",
    "> Use the visualization methods in the [second article above](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/) to make a scree plot of the principal components in `pca_full`. Use the \"find-the-elbow\" method from the [first article](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c) to find an optimal number of principal components. Strip down your `pca_full` dataset to just these first few columns, and save it as `pca_reduced.csv`.  \n",
    "\n",
    "> Train a linear model similar to the one you used in `kaggle1` on `pca_reduced`. How does this PCA regression model perform in comparison to your naive linear regression model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline\n",
    "\n",
    "#Load data set\n",
    "data = pd.read_csv('DATA/house-prices/train_cleaned.csv')\n",
    "Y = data['SalePrice']\n",
    "data_copy = data.drop(['SalePrice', 'Fence', 'GarageYrBlt'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_copy['MasVnrArea'])):\n",
    "    if pd.isna(data_copy['MasVnrArea'][i]):\n",
    "        data_copy['MasVnrArea'][i] = 0\n",
    "LotFrontAvg = np.mean(data_copy['LotFrontage'])\n",
    "for i in range(len(data_copy['LotFrontage'])):\n",
    "    if pd.isna(data_copy['LotFrontage'][i]):\n",
    "        data_copy['LotFrontage'][i] = LotFrontAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_copy.values\n",
    "X = scale(X)\n",
    "# pca = PCA(n_components=229)\n",
    "# pca.fit(X)\n",
    "# var= pca.explained_variance_ratio_\n",
    "# var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "# pca = PCA(n_components=30)\n",
    "# pca.fit(X)\n",
    "# X1=pca.fit_transform(X)\n",
    "# table = Table().with_columns('PCA', np.arange(1, 230), 'Variables', var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>...</th>\n",
       "      <th>Exterior2nd_VinylSd</th>\n",
       "      <th>Exterior2nd_Wd Sdng</th>\n",
       "      <th>Exterior2nd_Wd Shng</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "      <th>MSZoning_C (all)</th>\n",
       "      <th>MSZoning_FV</th>\n",
       "      <th>MSZoning_RH</th>\n",
       "      <th>MSZoning_RL</th>\n",
       "      <th>MSZoning_RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PC-1</th>\n",
       "      <td>-0.000521</td>\n",
       "      <td>-0.018934</td>\n",
       "      <td>0.080128</td>\n",
       "      <td>0.044413</td>\n",
       "      <td>-0.075913</td>\n",
       "      <td>0.006142</td>\n",
       "      <td>0.199806</td>\n",
       "      <td>-0.051907</td>\n",
       "      <td>0.206724</td>\n",
       "      <td>0.165304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126603</td>\n",
       "      <td>-0.079979</td>\n",
       "      <td>-0.020453</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>-0.035958</td>\n",
       "      <td>0.038053</td>\n",
       "      <td>-0.030078</td>\n",
       "      <td>0.084919</td>\n",
       "      <td>-0.102241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC-2</th>\n",
       "      <td>0.002258</td>\n",
       "      <td>-0.140688</td>\n",
       "      <td>0.080005</td>\n",
       "      <td>0.099072</td>\n",
       "      <td>-0.055342</td>\n",
       "      <td>-0.088157</td>\n",
       "      <td>-0.084302</td>\n",
       "      <td>0.043741</td>\n",
       "      <td>-0.001754</td>\n",
       "      <td>-0.091303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127509</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>-0.022297</td>\n",
       "      <td>-0.018758</td>\n",
       "      <td>0.018758</td>\n",
       "      <td>-0.017372</td>\n",
       "      <td>-0.131967</td>\n",
       "      <td>-0.031690</td>\n",
       "      <td>0.192386</td>\n",
       "      <td>-0.130849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PC-3</th>\n",
       "      <td>0.000411</td>\n",
       "      <td>-0.069519</td>\n",
       "      <td>0.187769</td>\n",
       "      <td>0.176365</td>\n",
       "      <td>-0.089354</td>\n",
       "      <td>-0.103506</td>\n",
       "      <td>0.067423</td>\n",
       "      <td>0.029894</td>\n",
       "      <td>-0.142598</td>\n",
       "      <td>-0.063813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082622</td>\n",
       "      <td>0.099846</td>\n",
       "      <td>0.017867</td>\n",
       "      <td>-0.007916</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>0.028296</td>\n",
       "      <td>-0.092651</td>\n",
       "      <td>0.016671</td>\n",
       "      <td>0.060101</td>\n",
       "      <td>-0.026684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 229 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  MSSubClass  LotFrontage   LotArea  LotShape  LandSlope  \\\n",
       "PC-1   -0.000521   -0.018934     0.080128  0.044413 -0.075913   0.006142   \n",
       "PC-2    0.002258   -0.140688     0.080005  0.099072 -0.055342  -0.088157   \n",
       "PC-3    0.000411   -0.069519     0.187769  0.176365 -0.089354  -0.103506   \n",
       "\n",
       "      OverallQual  OverallCond  YearBuilt  YearRemodAdd     ...       \\\n",
       "PC-1     0.199806    -0.051907   0.206724      0.165304     ...        \n",
       "PC-2    -0.084302     0.043741  -0.001754     -0.091303     ...        \n",
       "PC-3     0.067423     0.029894  -0.142598     -0.063813     ...        \n",
       "\n",
       "      Exterior2nd_VinylSd  Exterior2nd_Wd Sdng  Exterior2nd_Wd Shng  \\\n",
       "PC-1             0.126603            -0.079979            -0.020453   \n",
       "PC-2            -0.127509             0.008038            -0.022297   \n",
       "PC-3            -0.082622             0.099846             0.017867   \n",
       "\n",
       "      Utilities_AllPub  Utilities_NoSeWa  MSZoning_C (all)  MSZoning_FV  \\\n",
       "PC-1          0.000110         -0.000110         -0.035958     0.038053   \n",
       "PC-2         -0.018758          0.018758         -0.017372    -0.131967   \n",
       "PC-3         -0.007916          0.007916          0.028296    -0.092651   \n",
       "\n",
       "      MSZoning_RH  MSZoning_RL  MSZoning_RM  \n",
       "PC-1    -0.030078     0.084919    -0.102241  \n",
       "PC-2    -0.031690     0.192386    -0.130849  \n",
       "PC-3     0.016671     0.060101    -0.026684  \n",
       "\n",
       "[3 rows x 229 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit_transform(X)\n",
    "pd.DataFrame(pca.components_,columns=features,index=['PC-1','PC-2','PC-3'])\n",
    "\n",
    "\n",
    "# scores = pca2.transform(X)\n",
    "# reconstruct = pca.inverse_transform(scores)\n",
    "# reconstruct.shape\n",
    "\n",
    "\n",
    "# residual=data_copy-reconstruct\n",
    "# residual\n",
    "# var= pca.explained_variance_ratio_\n",
    "# var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "# pca.explained_variance_ratio_\n",
    "# new_dataframe = pd.DataFrame(data = pca2, index = [i for i in np.arange(1,96)], columns = [i for i in np.arange(1,96)])\n",
    "# pca2.to_csv('DATA/house-prices/pca_reduced.csv')\n",
    "# plt.plot(var1)\n",
    "# new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'GdPrv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-71b80a1addc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Scaling the values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mscale\u001b[0;34m(X, axis, with_mean, with_std, copy)\u001b[0m\n\u001b[1;32m    131\u001b[0m     X = check_array(X, accept_sparse='csc', copy=copy, ensure_2d=False,\n\u001b[1;32m    132\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'the scale function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'GdPrv'"
     ]
    }
   ],
   "source": [
    "#convert it to numpy arrays\n",
    "X = data_copy.values\n",
    "\n",
    "#Scaling the values\n",
    "X = scale(X)\n",
    "\n",
    "pca = PCA(n_components=232)\n",
    "\n",
    "pca.fit(X)\n",
    "\n",
    "#The amount of variance that each PC explains\n",
    "var= pca.explained_variance_ratio_\n",
    "\n",
    "#Cumulative Variance explains\n",
    "var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "\n",
    "#Looking at above plot I'm taking 30 variables\n",
    "pca = PCA(n_components=30)\n",
    "pca.fit(X)\n",
    "X1=pca.fit_transform(X)\n",
    "\n",
    "print(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "# Conclusion\n",
    "\n",
    "This ends the second of our four collaborative sessions on the Kaggle Housing Prices competition. We are so hyped for the SUSA Datathon this weekend, and hope to see you all there! As always, please email [Arun Ramamurthy](mailto:contact@arun.run), [Patrick Chao](mailto:prc@berkeley.edu), or [Noah Gundotra](mailto:noah.gundotra@berkeley.edu) or with any questions or concerns whatsoever. Happy machine learning!\n",
    "\n",
    "## Sneakpeek at SUSA Kaggle Competition III\n",
    "\n",
    "After the datathon, we will teach you some new models (e.g. decision trees, ensemble learning, and more) and give you practice reading **kernels** from Kaggle. This will help give you practice with new models for the final week, on using a deep learning model to improve your team's model performance even further!\n",
    "\n",
    "<a id='reading'></a>\n",
    "# Additional Reading\n",
    "* For more information on the Kaggle API, a command-line program used to download and manage Kaggle datasets, visit the [Kaggle API Github page](https://github.com/Kaggle/kaggle-api)  \n",
    "* For an interactive guide to learning R and Python, visit [DataCamp](https://www.datacamp.com/) a paid tutorial website for learning data computing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
